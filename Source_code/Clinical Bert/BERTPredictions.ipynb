{"cells":[{"cell_type":"code","execution_count":null,"id":"6797092f-3e5c-4bef-8f95-a14901e02378","metadata":{"id":"6797092f-3e5c-4bef-8f95-a14901e02378"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2LMHeadModel\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","import pandas as pd\n","from imblearn.over_sampling import RandomOverSampler\n","import pickle\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"id":"3cc0ed6d-fdfb-4a42-b964-cc1f319910b2","metadata":{"id":"3cc0ed6d-fdfb-4a42-b964-cc1f319910b2"},"outputs":[],"source":["with open('BERT_model_final.pkl', 'rb') as file:\n","    model = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"id":"a1b95c0d-52e4-4fd9-9139-2dcc69660e52","metadata":{"id":"a1b95c0d-52e4-4fd9-9139-2dcc69660e52"},"outputs":[],"source":["with open(\"tokenizedBERT.pkl\", \"rb\") as f:\n","    all_tokenized = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"9acb0878-82c8-4bdf-a981-053e230aa915","metadata":{"id":"9acb0878-82c8-4bdf-a981-053e230aa915","outputId":"07921bef-fe32-4f3c-bbfd-d927918ff633"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenized Query: {'input_ids': tensor([[  101,  1175,  1132, 14516, 15339,  1162,  8006,  1104, 10880,  1229,\n","         15207,   117,  1114,  1892,  2997,  1104,  2908,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["def tokenize_query(query_text, tokenizer_name='emilyalsentzer/Bio_ClinicalBERT', max_length=128):\n","\n","    tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n","\n","    inputs = tokenizer.encode_plus(\n","        query_text,\n","        None,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_token_type_ids=True,\n","        return_tensors='pt'\n","    )\n","\n","    tokenized_query = {\n","        'input_ids': inputs['input_ids'],\n","        'attention_mask': inputs['attention_mask'],\n","    }\n","\n","    return tokenized_query\n","\n","query_text = \"There are seviere symptoms of fever while discharged , with blood pressure of 80\"\n","tokenized_query = tokenize_query(query_text)\n","print(\"Tokenized Query:\", tokenized_query)\n"]},{"cell_type":"code","execution_count":null,"id":"735a8f86-cea2-4f8a-8699-94a59bc89b64","metadata":{"id":"735a8f86-cea2-4f8a-8699-94a59bc89b64","outputId":"ad997090-3f73-4782-c146-b9450844f443"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 Similar Records:\n","Record 1: {'input_ids': tensor([[  101,  1120, 13119,  ...,  4980, 14452,   102],\n","        [  101, 19538, 14541,  ...,  1394,  1818,   102],\n","        [  101, 22196,  1545,  ..., 12754,  3324,   102],\n","        ...,\n","        [  101, 14402,  1358,  ...,  4578, 16936,   102],\n","        [  101, 21692, 20581,  ...,  3862,  1527,   102],\n","        [  101,   175,  1181,  ..., 11109,  4351,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')}\n","Record 2: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1161,  5855,   102],\n","        [  101, 22723,  1604,  ..., 14541,  1545,   102],\n","        [  101, 22196, 18202,  ..., 12602,   174,   102],\n","        ...,\n","        [  101, 22148,  1545,  ...,  1200, 19310,   102],\n","        [  101, 10423,  1568,  ..., 10423,  1568,   102],\n","        [  101, 21692,  1545,  ...,  1673,  7637,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')}\n","Record 3: {'input_ids': tensor([[  101, 21692, 19598,  ...,  9455,  2050,   102],\n","        [  101, 19538, 24766,  ...,   172,  1777,   102],\n","        [  101, 14402,  1358,  ...,   172,  1182,   102],\n","        ...,\n","        [  101,   174,  1665,  ..., 22572,  2087,   102],\n","        [  101, 21640, 24400,  ..., 11263, 19852,   102],\n","        [  101, 12444,  2846,  ...,  1361,  6795,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')}\n","Record 4: {'input_ids': tensor([[  101, 11850,  1361,  ...,  1197,   177,   102],\n","        [  101, 22723, 18202,  ...,  6617, 11531,   102],\n","        [  101, 11850,  1361,  ...,  2373,   188,   102],\n","        ...,\n","        [  101, 11850,  1361,  ...,  1643,  2180,   102],\n","        [  101, 22588, 25631,  ..., 24312, 27291,   102],\n","        [  101, 21692, 11964,  ..., 14196, 19852,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')}\n","Record 5: {'input_ids': tensor([[  101, 11850,  1361,  ...,  1197,   177,   102],\n","        [  101, 22723, 17175,  ..., 14794,  5822,   102],\n","        [  101, 10296,  3805,  ..., 21461,  2569,   102],\n","        ...,\n","        [  101, 11138,  5890,  ...,  2255,  8179,   102],\n","        [  101, 18615, 21336,  ..., 10182,  6108,   102],\n","        [  101, 11850,  1361,  ..., 22371, 14255,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')}\n","Record 6: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1822, 20994,   102],\n","        [  101, 22723, 25491,  ...,  3367,  2629,   102],\n","        [  101, 18615, 22639,  ...,  2489,  2704,   102],\n","        ...,\n","        [  101,  5351, 13053,  ...,  2060,  4265,   102],\n","        [  101, 22723,  1580,  ...,  9016,  3457,   102],\n","        [  101,  5890,  3805,  ...,  1285,  1385,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')}\n","Record 7: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1822, 20994,   102],\n","        [  101, 22723, 11964,  ...,  5851,  6006,   102],\n","        [  101, 22196, 25892,  ..., 26600, 26557,   102],\n","        ...,\n","        [  101, 18615, 14541,  ..., 11769,  7409,   102],\n","        [  101, 11850,  1361,  ..., 14235,  2773,   102],\n","        [  101, 24762, 10024,  ...,  3810, 26358,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')}\n","Record 8: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1822, 20994,   102],\n","        [  101, 22723,  1571,  ...,  1233,  6617,   102],\n","        [  101, 24762, 10024,  ...,  6779, 23447,   102],\n","        ...,\n","        [  101, 22723, 23435,  ..., 23057,  3507,   102],\n","        [  101, 19538, 14541,  ...,  1396,  1605,   102],\n","        [  101, 21692,  1559,  ...,  1762,  2060,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')}\n","Record 9: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1822, 20994,   102],\n","        [  101, 21640,  1545,  ...,  1119,  4163,   102],\n","        [  101, 22196, 22639,  ...,  8240,  1166,   102],\n","        ...,\n","        [  101, 24762, 10024,  ...,   125,  4375,   102],\n","        [  101, 20915, 26752,  ...,   123,  8080,   102],\n","        [  101, 24762, 10024,  ..., 10182,  1264,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')}\n","Record 10: {'input_ids': tensor([[  101,  1120, 13119,  ...,  1822, 20994,   102],\n","        [  101, 24762, 10024,  ...,   176, 16144,   102],\n","        [  101, 11437, 10182,  ...,  6779,  8277,   102],\n","        ...,\n","        [  101, 22148, 24786,  ...,  1145,  8318,   102],\n","        [  101, 18615,  1559,  ..., 24214,  5173,   102],\n","        [  101,  2705, 12522,  ...,  2728,  1233,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')}\n"]}],"source":["query_input_ids = tokenized_query['input_ids']\n","query_attention_mask = tokenized_query['attention_mask']\n","query_input_ids_np = query_input_ids.cpu().numpy().reshape(1, -1)\n","query_attention_mask_np = query_attention_mask.cpu().numpy().reshape(1, -1)\n","\n","similarities = []\n","for record in all_tokenized:\n","    record_input_ids = record['input_ids']\n","    record_attention_mask = record['attention_mask']\n","\n","\n","    record_input_ids_np = record_input_ids.cpu().numpy().reshape(1, -1)\n","    record_attention_mask_np = record_attention_mask.cpu().numpy().reshape(1, -1)\n","\n","\n","    record_input_ids_np = record_input_ids_np[:, :query_input_ids_np.shape[1]]\n","    record_attention_mask_np = record_attention_mask_np[:, :query_attention_mask_np.shape[1]]\n","\n","\n","    sim = cosine_similarity(query_input_ids_np, record_input_ids_np)\n","    similarities.append(sim)\n","\n","similarities_array = np.array(similarities)\n","top_10_indices = similarities_array.argsort(axis=None)[-10:][::-1]\n","top_10_records = [all_tokenized[idx] for idx in top_10_indices]\n","\n","print(\"Top 10 Similar Records:\")\n","for i, record in enumerate(top_10_records, start=1):\n","    print(f\"Record {i}: {record}\")"]},{"cell_type":"code","execution_count":null,"id":"3131eac1-7392-4a7f-b6b2-23fdff075419","metadata":{"id":"3131eac1-7392-4a7f-b6b2-23fdff075419"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"278a6e2d-bf50-4b38-b7b1-5563a1e81f09","metadata":{"id":"278a6e2d-bf50-4b38-b7b1-5563a1e81f09","outputId":"e3767008-eefc-4ee9-c180-868892280380"},"outputs":[{"name":"stdout","output_type":"stream","text":["Readmission Risk within 30 days\n"]}],"source":["def predict_record(record):\n","\n","    input_ids = record['input_ids'].to(device)\n","    attention_mask = record['attention_mask'].to(device)\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        _, prediction = torch.max(outputs.logits, dim=1)\n","\n","    return prediction\n","\n","def predict_records(records):\n","    predictions = []\n","    for record in records:\n","        prediction = predict_record(record)\n","        predictions.extend(prediction.tolist())\n","\n","    counts = torch.bincount(torch.tensor(predictions))\n","\n","    majority_prediction = torch.argmax(counts)\n","    return majority_prediction.item()\n","\n","\n","final_prediction = predict_records(top_10_records)\n","\n","if final_prediction == 0:\n","    print(\"No Readmission Risk within 30 days\")\n","elif final_prediction == 1:\n","    print(\"Readmission Risk within 30 days\")"]},{"cell_type":"code","execution_count":null,"id":"d5c37bd8-3b4b-4e90-a78f-22a568edcc25","metadata":{"id":"d5c37bd8-3b4b-4e90-a78f-22a568edcc25","outputId":"be103189-9b1e-48a2-ca47-ec94d0b87a34"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","generator_tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","\n","def convert_tokenized_query_to_text(tokenized_query):\n","    input_ids = tokenized_query['input_ids'].tolist()[0]\n","    text = generator_tokenizer.decode(input_ids, skip_special_tokens=True)\n","    return text\n","\n","def convert_records_to_text(records):\n","    texts = []\n","    for record in records[:1]:\n","        input_ids = record['input_ids'].tolist()[0]\n","        text = generator_tokenizer.decode(input_ids, skip_special_tokens=True)\n","        texts.append(text)\n","    return texts\n","\n","\n","def generate_input_for_gpt2(query_text, prediction_result):\n","    combined_text = query_text + \". \"\n","    combined_text += \". Prediction: \" + prediction_result\n","    return combined_text\n","\n","retrieved_texts = convert_records_to_text(top_10_records)\n","query_text_decoded = convert_tokenized_query_to_text(tokenized_query)"]},{"cell_type":"code","execution_count":null,"id":"7909216f-d0b7-4790-8f1d-1250bef569af","metadata":{"id":"7909216f-d0b7-4790-8f1d-1250bef569af","outputId":"5136b9da-5f81-4de3-d710-42e3648a98dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["there are seviere symptoms of fever while discharged, with blood pressure of 80.. Prediction: Readmission Risk within 30 days of discharge.\n","\n","The following are the symptoms of seviere:\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n","\n","Severe headache\n"]}],"source":["prediction_result = \"No Readmission Risk within 30 days\" if final_prediction == 0 else \"Readmission Risk within 30 days\"\n","\n","input_for_gpt2 = generate_input_for_gpt2(query_text_decoded, prediction_result)\n","\n","input_ids = gpt2_tokenizer.encode(input_for_gpt2, return_tensors='pt')\n","\n","output = gpt2_model.generate(\n","    input_ids,\n","    max_new_tokens=300,\n","    num_return_sequences=1,\n","    temperature=0.7,\n","    repetition_penalty=1.0,\n","    top_k=100,\n","    top_p=0.95\n",")\n","\n","generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}